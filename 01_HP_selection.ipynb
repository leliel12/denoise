{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sns.set()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import joblib\n",
    "\n",
    "from libs.container import Container\n",
    "from libs.display import d\n",
    "from libs.experiment import KFoldExperiment, WithAnotherExperiment, roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = joblib.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_pickle(\"data/scaled/sample.pkl.bz2\")\n",
    "sample[\"tile\"] = sample[\"id\"].apply(lambda i: \"b\" + str(i)[1:4])\n",
    "\n",
    "no_features = [\"id\", \"vs_catalog\", \"vs_type\", \"ra_k\", \"dec_k\", \"tile\", \"cls\"] \n",
    "X_columns = [c for c in sample.columns if c not in no_features]\n",
    "\n",
    "grouped = sample.groupby(\"tile\")\n",
    "data = Container({k: grouped.get_group(k).copy() for k in grouped.groups.keys()})\n",
    "\n",
    "del grouped, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b261': 1, 'b278': 0}\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([data.b278, data.b261])\n",
    "\n",
    "cls = {name: idx for idx, name in enumerate(df.tile.unique())}\n",
    "df[\"cls\"] = df.tile.apply(cls.get)\n",
    "\n",
    "print(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[X_columns].values\n",
    "y = df.cls.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.652 (+/-0.037) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}\n",
      "0.598 (+/-0.106) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}\n",
      "0.676 (+/-0.073) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}\n",
      "0.650 (+/-0.039) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}\n",
      "0.698 (+/-0.056) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}\n",
      "0.670 (+/-0.069) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}\n",
      "0.743 (+/-0.044) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
      "0.689 (+/-0.052) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}\n",
      "0.685 (+/-0.061) for {'kernel': 'linear', 'C': 1}\n",
      "0.701 (+/-0.045) for {'kernel': 'linear', 'C': 10}\n",
      "0.716 (+/-0.048) for {'kernel': 'linear', 'C': 100}\n",
      "0.712 (+/-0.049) for {'kernel': 'linear', 'C': 1000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.70      0.73       215\n",
      "           1       0.68      0.75      0.71       184\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       399\n",
      "   macro avg       0.72      0.72      0.72       399\n",
      "weighted avg       0.73      0.72      0.72       399\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.636 (+/-0.037) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}\n",
      "0.517 (+/-0.023) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}\n",
      "0.673 (+/-0.076) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}\n",
      "0.634 (+/-0.042) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}\n",
      "0.695 (+/-0.058) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}\n",
      "0.667 (+/-0.071) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}\n",
      "0.741 (+/-0.050) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
      "0.686 (+/-0.053) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}\n",
      "0.681 (+/-0.062) for {'kernel': 'linear', 'C': 1}\n",
      "0.698 (+/-0.047) for {'kernel': 'linear', 'C': 10}\n",
      "0.713 (+/-0.050) for {'kernel': 'linear', 'C': 100}\n",
      "0.709 (+/-0.049) for {'kernel': 'linear', 'C': 1000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.70      0.73       215\n",
      "           1       0.68      0.75      0.71       184\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       399\n",
      "   macro avg       0.72      0.72      0.72       399\n",
      "weighted avg       0.73      0.72      0.72       399\n",
      "\n",
      "\n",
      "CPU times: user 4.64 s, sys: 1.47 s, total: 6.1 s\n",
      "Wall time: 19min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [\n",
    "    {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
    "    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}\n",
    "]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5, n_jobs=cpu,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7.5498344352707498]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(5, 15)) + [np.sqrt(len(X_columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_features': None, 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.811 (+/-0.058) for {'max_features': 'auto', 'min_samples_split': 2, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.806 (+/-0.045) for {'max_features': 'auto', 'min_samples_split': 5, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.809 (+/-0.049) for {'max_features': 'auto', 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.811 (+/-0.056) for {'max_features': 'sqrt', 'min_samples_split': 2, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.812 (+/-0.047) for {'max_features': 'sqrt', 'min_samples_split': 5, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.814 (+/-0.048) for {'max_features': 'sqrt', 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.806 (+/-0.045) for {'max_features': 'log2', 'min_samples_split': 2, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.802 (+/-0.039) for {'max_features': 'log2', 'min_samples_split': 5, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.806 (+/-0.044) for {'max_features': 'log2', 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.858 (+/-0.027) for {'max_features': None, 'min_samples_split': 2, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.855 (+/-0.031) for {'max_features': None, 'min_samples_split': 5, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.858 (+/-0.031) for {'max_features': None, 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.829 (+/-0.050) for {'max_features': 0.2, 'min_samples_split': 2, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.828 (+/-0.049) for {'max_features': 0.2, 'min_samples_split': 5, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.827 (+/-0.052) for {'max_features': 0.2, 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.851 (+/-0.041) for {'max_features': 0.5, 'min_samples_split': 2, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.852 (+/-0.039) for {'max_features': 0.5, 'min_samples_split': 5, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.853 (+/-0.041) for {'max_features': 0.5, 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89       215\n",
      "           1       0.87      0.86      0.87       184\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       399\n",
      "   macro avg       0.88      0.88      0.88       399\n",
      "weighted avg       0.88      0.88      0.88       399\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_features': None, 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.814 (+/-0.052) for {'max_features': 'auto', 'min_samples_split': 2, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.811 (+/-0.050) for {'max_features': 'auto', 'min_samples_split': 5, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.804 (+/-0.050) for {'max_features': 'auto', 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.817 (+/-0.051) for {'max_features': 'sqrt', 'min_samples_split': 2, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.809 (+/-0.050) for {'max_features': 'sqrt', 'min_samples_split': 5, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.814 (+/-0.053) for {'max_features': 'sqrt', 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.800 (+/-0.031) for {'max_features': 'log2', 'min_samples_split': 2, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.804 (+/-0.046) for {'max_features': 'log2', 'min_samples_split': 5, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.796 (+/-0.047) for {'max_features': 'log2', 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.854 (+/-0.030) for {'max_features': None, 'min_samples_split': 2, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.856 (+/-0.029) for {'max_features': None, 'min_samples_split': 5, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.861 (+/-0.035) for {'max_features': None, 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.823 (+/-0.043) for {'max_features': 0.2, 'min_samples_split': 2, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.827 (+/-0.047) for {'max_features': 0.2, 'min_samples_split': 5, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.823 (+/-0.040) for {'max_features': 0.2, 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.852 (+/-0.038) for {'max_features': 0.5, 'min_samples_split': 2, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.852 (+/-0.043) for {'max_features': 0.5, 'min_samples_split': 5, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "0.856 (+/-0.039) for {'max_features': 0.5, 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89       215\n",
      "           1       0.87      0.87      0.87       184\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       399\n",
      "   macro avg       0.88      0.88      0.88       399\n",
      "weighted avg       0.88      0.88      0.88       399\n",
      "\n",
      "\n",
      "CPU times: user 2min 58s, sys: 1.1 s, total: 2min 59s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [\n",
    "    {'max_features': ['auto', 'sqrt', \"log2\", None, 0.2, 0.5], \n",
    "     \"min_samples_split\": [2, 5, 10],\n",
    "     \"n_estimators\": [500], \n",
    "     \"criterion\": [\"entropy\"], \n",
    "     \"n_jobs\": [10]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5, n_jobs=cpu,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest \n",
    "RF_prec = {'max_features': None, 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
    "RF_recall = {'max_features': None, 'min_samples_split': 10, 'n_jobs': 10, 'criterion': 'entropy', 'n_estimators': 500}\n",
    "\n",
    "RF_prec == RF_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_prec = {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
    "SVM_recall = {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
    "\n",
    "SVM_recall == SVM_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
